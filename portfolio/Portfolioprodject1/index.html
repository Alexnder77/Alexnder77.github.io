<!doctype html><html><head><style>.header{position:absolute;top:0;left:0;width:100%;height:250px;background-image:url(/portfolio_assets/images/01.jpg);background-repeat:no-repeat;background-size:cover;transition:background-color .3s ease-in-out;box-shadow:0 2px 4px rgba(0,0,0,.1)}.navbar2{background-color:var(--light);padding:0 2em;display:flex;align-items:center;justify-content:space-between}.navbar2-menu{list-style:none;margin:0;padding:0;display:flex;display:flex}.navbar2-menu li{margin-left:20px}.navbar2-menu li a{color:#fff;text-decoration:none;font-size:16px;display:flex}.navbar2-menu li a:hover{text-decoration:underline}body{justify-content:center;align-items:center;margin:0;padding-top:150px}.container{display:flex;position:relative;justify-content:center;flex-direction:row;flex-wrap:nowrap;align-items:start;padding:1em 8em 0;margin:0 auto;top:0}.singlePage2{flex:1;max-width:900px;min-width:500px;padding:15px 25px 25px;background:var(--light);border-radius:15px;box-shadow:0 2px 4px rgba(0,0,0,.1)}.tableOfContentContainer{flex:1;padding-top:100px;padding-left:25px}@media all and (max-width:750px){.tableOfContentContainer{display:none}.container{padding:1em}.singlePage2{min-width:-webkit-fill-available}}</style><head><meta charset=utf-8><meta name=description content="Introduction Support Vector Machines (SVMs) are powerful machine learning models that can be used for both classification and regression tasks. In classification, the goal is to find a hyperplane that separates the data points of different classes with maximum margin."><meta property="og:title" content="Hyperplane in SVM"><meta property="og:description" content="Introduction Support Vector Machines (SVMs) are powerful machine learning models that can be used for both classification and regression tasks. In classification, the goal is to find a hyperplane that separates the data points of different classes with maximum margin."><meta property="og:type" content="website"><meta property="og:image" content="https://alexnder77.github.io/icon.png"><meta property="og:url" content="https://alexnder77.github.io/portfolio/Portfolioprodject1/"><meta property="og:width" content="200"><meta property="og:height" content="200"><meta name=twitter:card content="summary"><meta name=twitter:title content="Hyperplane in SVM"><meta name=twitter:description content="Introduction Support Vector Machines (SVMs) are powerful machine learning models that can be used for both classification and regression tasks. In classification, the goal is to find a hyperplane that separates the data points of different classes with maximum margin."><meta name=twitter:image content="https://alexnder77.github.io/icon.png"><meta name=twitter:site content="alexand27264042"><title>Hyperplane in SVM</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://alexnder77.github.io//icon.png><link href=https://alexnder77.github.io/styles.329c413f393c99f25710bf1b9bb7dbc3.min.css rel=stylesheet><link href=https://alexnder77.github.io/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://alexnder77.github.io/js/darkmode.f73aec5dc5664d36fb5aa59a0daa1aca.min.js></script>
<script src=https://alexnder77.github.io/js/util.00639692264b21bc3ee219733d38a8be.min.js></script>
<link rel=preload href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css as=style onload='this.onload=null,this.rel="stylesheet"' integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/core@1.2.1></script>
<script src=https://cdn.jsdelivr.net/npm/@floating-ui/dom@1.2.1></script>
<script defer src=https://alexnder77.github.io/js/popover.aa9bc99c7c38d3ae9538f218f1416adb.min.js></script>
<script defer src=https://alexnder77.github.io/js/code-title.ce4a43f09239a9efb48fee342e8ef2df.min.js></script>
<script defer src=https://alexnder77.github.io/js/clipboard.2913da76d3cb21c5deaa4bae7da38c9f.min.js></script>
<script defer src=https://alexnder77.github.io/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const SEARCH_ENABLED=!1,LATEX_ENABLED=!0,PRODUCTION=!0,BASE_URL="https://alexnder77.github.io/",fetchData=Promise.all([fetch("https://alexnder77.github.io/indices/linkIndex.d8802eb80bdbd40492c37715d6796f06.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://alexnder77.github.io/indices/contentIndex.9c5d54ed50b629c5a501d2a808662f67.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://alexnder77.github.io",!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://alexnder77.github.io",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}var i=document.getElementsByClassName("mermaid");i.length>0&&import("https://unpkg.com/mermaid@9/dist/mermaid.esm.min.mjs").then(e=>{e.default.init()});function a(n){const e=n.target,t=e.className.split(" "),s=t.includes("broken"),o=t.includes("internal-link");plausible("Link Click",{props:{href:e.href,broken:s,internal:o,graph:!1}})}const r=document.querySelectorAll("a");for(link of r)link.className.includes("root-title")&&link.addEventListener("click",a,{once:!0})},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'’':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/alexnder77.github.io\/js\/router.d6fe6bd821db9ea97f9aeefae814d8e7.min.js"
    attachSPARouting(init, render)
  </script><script defer data-domain=alexnder77.github.io src=https://plausible.io/js/script.js></script>
<script>window.plausible=window.plausible||function(){(window.plausible.q=window.plausible.q||[]).push(arguments)}</script></head></head><body><div class=header id=header><nav class=navbar2 id=navbar2><a class="image avatar"><img src=/portfolio_assets/images/logo.png alt="cant find image" width=50 height=50 border-radius=5px box-shadow="0 2px 4px 0.1" margin=0></a><ul class=navbar2-menu><li><a href=/portfolio>Portfolio Home</a></li><li><a href=/portfolio/about>About</a></li><li><a href=/notes/>Notes</a></li></ul></nav></div><div class=container><div class=singlePage2><header><h1 id=page-title><a class=root-title>Hyperplane in SVM</a></h1><div class=spacer></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><a href=#introduction><h2 id=introduction><span class=hanchor arialabel=Anchor># </span>Introduction</h2></a><p>Support Vector Machines (SVMs) are powerful machine learning models that can be used for both classification and regression tasks. In classification, the goal is to find a hyperplane that separates the data points of different classes with maximum margin. This hyperplane is known as the &ldquo;optimal hyperplane&rdquo; or &ldquo;maximum-margin hyperplane&rdquo;.</p><p>To understand the concept of the hyperplane in SVMs, let&rsquo;s dive deeper into the working of SVMs and how the optimal hyperplane is determined.</p><a href=#support-vector-machines><h2 id=support-vector-machines><span class=hanchor arialabel=Anchor># </span>Support Vector Machines</h2></a><p>The idea behind SVMs is to find a plane or a boundary that effectively separates the data points of different classes. For example in a binary classification problem where we have 2 different classes and the data is 2-dimensional. It can be separated with a line.</p><p>Here we can see a line separating the red from the blue dots the color representing the two different classes.<br><a class="internal-link broken">Pasted image 20230619135001.png</a></p><p>If the data has 3 or more dimensions it needs to be separated by a hyperplane. which can be illustrated like this:
<a class="internal-link broken">Pasted image 20230619133752.png</a></p><p>This principle remains even in higher dimensions, but illustrating them is harder.</p><p>(Note: technically speaking all flat affine subspaces are hyperplanes so the 1 dimensional line from image 1 is also a hyperplane)</p><a href=#finding-the-best-hyperplane><h2 id=finding-the-best-hyperplane><span class=hanchor arialabel=Anchor># </span>Finding the best hyperplane</h2></a><p>The data points that lie closest to the hyperplane/the decision boundary are called support vectors. These support vectors play a crucial role in determining the optimal hyperplane. The distance between the hyperplane and the support vectors is known as the margin. The SVM tries to maximize this margin, as it provides a measure of the confidence of classification. In a sense the longer the distance between the different classes the more clearly they are separated and the better the model is.</p><p><a class="internal-link broken">Drawing 2023-06-20 12.05.43.excalidraw</a>
The hyperplane can be represented by the equation:
$$y = w^T x + b$$
Where $y$ represents the label, $w$ and $b$ represents the parameters of the hyperplane. For our binary classification problem we only have the classes -1 and 1. So $y \in \begin{Bmatrix} 1, -1 \end{Bmatrix}$.</p><p>Since the hyperplane represents the decision boundary, any point on the hyperplane will have to fit this equation $w^T x + b = 0$. This is in the middle of the 2 classes, which can be interpreted as we don&rsquo;t know what class that point belongs to.</p><p>The decision rule is based on the sign of the equation above. If $w^T \times x + b > 0$, the point $x$ is classified as class 1, and if $w^T \times x + b &lt; 0$, the point $x$ is classified as class -1.</p><p>The optimal hyperplane is the one that maximizes the margin between the two classes.</p><p>The margin is defined as the perpendicular distance between the hyperplane and the closest data points from each class. These closest data points are called support vectors.</p><p>Let&rsquo;s denote the support vectors as $x_+$for class 1 and $x_-$ for class -1. The distance between these support vectors is given by:
$$\text{margin} = \frac{2}{|w|}$$ where $|w|$ represents the Euclidean norm of the weight vector $w$. The goal of SVM is to find the hyperplane that maximizes this margin, which in turn improves the generalization performance of the classifier.</p><p>To find the optimal hyperplane, SVM solves either the following maximization or the equivalent minimization -problem an optimization problem:
$$\max_{w,b} = \frac{2}{|w|} \Leftrightarrow \min_{w,b} \frac{1}{2} |w|^2$$
subject to the constraints: $$y_i(w^T x_i + b) \geq 1 \text{ for all i} = 1,2,&mldr;,N$$
Here, $N$ represents the number of data points, and $y_i$ represents the class labels. The constraints ensure that all data points are correctly classified and lie on the correct side of the margin.</p><p>The optimization problem can be solved using various algorithms, such as the Sequential Minimal Optimization (SMO) or the Interior Point Methods. Once the optimization problem is solved, the learned parameters $w$ and $b$ define the optimal hyperplane that separates the two classes with the maximum margin.</p><a href=#implementation><h2 id=implementation><span class=hanchor arialabel=Anchor># </span>Implementation</h2></a><p>using the above equations and applying it in python (the minimization problem is solved using the function minimize from the package scipy.optimize):</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span><span class=lnt>54
</span><span class=lnt>55
</span><span class=lnt>56
</span><span class=lnt>57
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Step 1: Define the dataset</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>],</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=p>[</span><span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>]])</span>  <span class=c1># Features</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>])</span>  <span class=c1># Labels</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Step 2: Define the SVM optimization problem</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>objective</span><span class=p>(</span><span class=n>w</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=mf>0.5</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>w</span><span class=p>)</span><span class=o>**</span><span class=mi>2</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>constraint</span><span class=p>(</span><span class=n>w</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>y</span> <span class=o>*</span> <span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>w</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span> <span class=o>+</span> <span class=n>w</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>])</span> <span class=o>-</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Step 3: Solve the SVM optimization problem</span>
</span></span><span class=line><span class=cl><span class=n>initial_guess</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span>  <span class=c1># Initial guess for w and b</span>
</span></span><span class=line><span class=cl><span class=n>bounds</span> <span class=o>=</span> <span class=p>[(</span><span class=kc>None</span><span class=p>,</span> <span class=kc>None</span><span class=p>)]</span> <span class=o>*</span> <span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=p>[(</span><span class=kc>None</span><span class=p>,</span> <span class=kc>None</span><span class=p>)]</span>  <span class=c1># No bounds on w and b</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>scipy.optimize</span> <span class=kn>import</span> <span class=n>minimize</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>solution</span> <span class=o>=</span> <span class=n>minimize</span><span class=p>(</span><span class=n>objective</span><span class=p>,</span> <span class=n>initial_guess</span><span class=p>,</span> <span class=n>bounds</span><span class=o>=</span><span class=n>bounds</span><span class=p>,</span> <span class=n>constraints</span><span class=o>=</span><span class=p>{</span><span class=s1>&#39;type&#39;</span><span class=p>:</span> <span class=s1>&#39;ineq&#39;</span><span class=p>,</span> <span class=s1>&#39;fun&#39;</span><span class=p>:</span> <span class=n>constraint</span><span class=p>})</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Step 4: Extract the optimal parameters</span>
</span></span><span class=line><span class=cl><span class=n>w</span> <span class=o>=</span> <span class=n>solution</span><span class=o>.</span><span class=n>x</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>solution</span><span class=o>.</span><span class=n>x</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Calculate the margin</span>
</span></span><span class=line><span class=cl><span class=n>margin</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>w</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plotting the results</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>c</span><span class=o>=</span><span class=n>y</span><span class=p>,</span> <span class=n>cmap</span><span class=o>=</span><span class=s1>&#39;bwr&#39;</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Dataset&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plotting the decision boundary</span>
</span></span><span class=line><span class=cl><span class=n>x_min</span><span class=p>,</span> <span class=n>x_max</span> <span class=o>=</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>min</span><span class=p>()</span> <span class=o>-</span> <span class=mi>1</span><span class=p>,</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>]</span><span class=o>.</span><span class=n>max</span><span class=p>()</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=n>y_min</span><span class=p>,</span> <span class=n>y_max</span> <span class=o>=</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>min</span><span class=p>()</span> <span class=o>-</span> <span class=mi>1</span><span class=p>,</span> <span class=n>X</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>]</span><span class=o>.</span><span class=n>max</span><span class=p>()</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=n>xx</span><span class=p>,</span> <span class=n>yy</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>meshgrid</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>x_min</span><span class=p>,</span> <span class=n>x_max</span><span class=p>,</span> <span class=mf>0.01</span><span class=p>),</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>y_min</span><span class=p>,</span> <span class=n>y_max</span><span class=p>,</span> <span class=mf>0.01</span><span class=p>))</span>
</span></span><span class=line><span class=cl><span class=n>grid_points</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>c_</span><span class=p>[</span><span class=n>xx</span><span class=o>.</span><span class=n>ravel</span><span class=p>(),</span> <span class=n>yy</span><span class=o>.</span><span class=n>ravel</span><span class=p>()]</span>
</span></span><span class=line><span class=cl><span class=n>decision_values</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>grid_points</span><span class=p>,</span> <span class=n>w</span><span class=p>)</span> <span class=o>+</span> <span class=n>b</span>
</span></span><span class=line><span class=cl><span class=n>Z</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sign</span><span class=p>(</span><span class=n>decision_values</span><span class=p>)</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>xx</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>contour</span><span class=p>(</span><span class=n>xx</span><span class=p>,</span> <span class=n>yy</span><span class=p>,</span> <span class=n>Z</span><span class=p>,</span> <span class=n>colors</span><span class=o>=</span><span class=s1>&#39;k&#39;</span><span class=p>,</span> <span class=n>levels</span><span class=o>=</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Plotting the margin</span>
</span></span><span class=line><span class=cl><span class=n>support_vectors</span> <span class=o>=</span> <span class=n>X</span><span class=p>[</span><span class=n>solution</span><span class=o>.</span><span class=n>success</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>support_vectors</span><span class=p>[:,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>support_vectors</span><span class=p>[:,</span> <span class=mi>1</span><span class=p>],</span> <span class=n>s</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>facecolors</span><span class=o>=</span><span class=s1>&#39;none&#39;</span><span class=p>,</span> <span class=n>edgecolors</span><span class=o>=</span><span class=s1>&#39;k&#39;</span><span class=p>,</span> <span class=n>linewidths</span><span class=o>=</span><span class=mi>2</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Support Vectors&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Setting plot limits and labels</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlim</span><span class=p>(</span><span class=n>x_min</span><span class=p>,</span> <span class=n>x_max</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylim</span><span class=p>(</span><span class=n>y_min</span><span class=p>,</span> <span class=n>y_max</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;X&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Y&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>title</span><span class=p>(</span><span class=s1>&#39;SVM Classification&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Display the margin</span>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>text</span><span class=p>(</span><span class=n>x_min</span> <span class=o>+</span> <span class=mf>0.1</span><span class=p>,</span> <span class=n>y_max</span> <span class=o>-</span> <span class=mf>0.1</span><span class=p>,</span> <span class=sa>f</span><span class=s1>&#39;Margin: </span><span class=si>{</span><span class=n>margin</span><span class=si>:</span><span class=s1>.2f</span><span class=si>}</span><span class=s1>&#39;</span><span class=p>,</span> <span class=n>fontsize</span><span class=o>=</span><span class=mi>12</span><span class=p>,</span> <span class=n>bbox</span><span class=o>=</span><span class=nb>dict</span><span class=p>(</span><span class=n>facecolor</span><span class=o>=</span><span class=s1>&#39;white&#39;</span><span class=p>,</span> <span class=n>edgecolor</span><span class=o>=</span><span class=s1>&#39;black&#39;</span><span class=p>,</span> <span class=n>boxstyle</span><span class=o>=</span><span class=s1>&#39;round&#39;</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>plt</span><span class=o>.</span><span class=n>show</span><span class=p>()</span>
</span></span></code></pre></td></tr></table></div></div><p><a class="internal-link broken">Pasted image 20230619174013.png</a>
<a class="internal-link broken">Pasted image 20230619174527.png</a>
comparing to the sklearn svm function:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sklearn.svm</span> <span class=kn>import</span> <span class=n>SVC</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Step 1: Define the dataset</span>
</span></span><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=p>[[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>],</span> <span class=p>[</span><span class=mi>2</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=p>[</span><span class=mi>3</span><span class=p>,</span> <span class=mi>2</span><span class=p>]]</span>  <span class=c1># Features</span>
</span></span><span class=line><span class=cl><span class=n>y</span> <span class=o>=</span> <span class=p>[</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>]</span>  <span class=c1># Labels</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Step 2: Create and fit the SVM classifier</span>
</span></span><span class=line><span class=cl><span class=n>clf</span> <span class=o>=</span> <span class=n>SVC</span><span class=p>(</span><span class=n>kernel</span><span class=o>=</span><span class=s1>&#39;linear&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>clf</span><span class=o>.</span><span class=n>fit</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Step 3: Calculate the weight vector, bias term, and margin</span>
</span></span><span class=line><span class=cl><span class=n>w</span> <span class=o>=</span> <span class=n>clf</span><span class=o>.</span><span class=n>coef_</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=n>clf</span><span class=o>.</span><span class=n>intercept_</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>margin</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>/</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>w</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Step 4: Print the results</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Weight Vector:&#34;</span><span class=p>,</span> <span class=n>w</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Bias Term:&#34;</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Margin:&#34;</span><span class=p>,</span> <span class=n>margin</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p><a class="internal-link broken">Pasted image 20230619174206.png</a></p><a href=#conclusion><h2 id=conclusion><span class=hanchor arialabel=Anchor># </span>Conclusion</h2></a><p>It is important to note that SVM is a linear classifier, which means it can only separate classes using linear decision boundaries. However, by applying different kernel functions, SVM can also handle non-linear decision boundaries by mapping the data into a higher-dimensional feature space.</p><p>In conclusion, the hyperplane in SVM represents the decision boundary that separates the two classes, and the optimal hyperplane is the one that maximizes the margin between the classes. SVM achieves this by solving an optimization problem and finding the best parameters that define the hyperplane.</p><p>SVM is a powerful and widely used algorithm in machine learning due to its ability to handle both linear and non-linear classification problems. It has proven to be effective in various applications, such as image classification, text categorization, and bioinformatics, among others.</p><a href=#references><h2 id=references><span class=hanchor arialabel=Anchor># </span>References</h2></a><p><a href=https://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-and-machine-learning-on-documents-1.html rel=noopener>Support vector machines and machine learning on documents</a>
<a href=https://www.geeksforgeeks.org/separating-hyperplanes-in-svm/ rel=noopener>Separating Hyperplanes in SVM - GeeksforGeeks</a></p></div><aside class=tableOfContentContainer id=tableOfContentContainer><h3>Table of Contents</h3><nav id=TableOfContents><ol><li><a href=#introduction>Introduction</a></li><li><a href=#support-vector-machines>Support Vector Machines</a></li><li><a href=#finding-the-best-hyperplane>Finding the best hyperplane</a></li><li><a href=#implementation>Implementation</a></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li></ol></nav></aside></div><div id=contact_buttons><footer><p>Made by Alexander Nilsson using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, © 2023</p><ul><li><a href=https://alexnder77.github.io/>Home</a></li><li><a href=https://twitter.com/alexand27264042>Twitter</a></li><li><a href=https://github.com/Alexnder77>GitHub</a></li></ul></footer></div><br><br><script>const header=document.getElementById("header"),navbar=document.getElementById("navbar"),headerImage=document.getElementById("headerImage"),expandIcon=document.getElementById("expandIcon");function handleScroll(){window.scrollY>0?(header.classList.add("scrolled"),navbar.style.display="none"):(header.classList.remove("scrolled"),navbar.style.display="block")}function toggleNavbar(){navbar.style.display=navbar.style.display==="block"?"none":"block"}window.addEventListener("scroll",handleScroll),expandIcon.addEventListener("click",toggleNavbar)</script></body></html>